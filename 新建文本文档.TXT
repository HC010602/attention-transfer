PS D:\desk\yan\code\attention-transfer> python cifar.py --save logs/resnet_16_2_teacher --depth 16 --width 2 --cuda
parsed options: {'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.
2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0'}
Files already downloaded and verified
Files already downloaded and verified
creating optimizer with lr =  0.1

Parameters:
0     conv0                             (16, 3, 3, 3)           torch.cuda.FloatTensor True
1     group0.block0.conv0               (32, 16, 3, 3)          torch.cuda.FloatTensor True
2     group0.block0.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor True
3     group0.block0.bn0.weight          (16,)                   torch.cuda.FloatTensor True
4     group0.block0.bn0.bias            (16,)                   torch.cuda.FloatTensor True
5     group0.block0.bn0.running_mean    (16,)                   torch.cuda.FloatTensor False
6     group0.block0.bn0.running_var     (16,)                   torch.cuda.FloatTensor False
7     group0.block0.bn1.weight          (32,)                   torch.cuda.FloatTensor True
8     group0.block0.bn1.bias            (32,)                   torch.cuda.FloatTensor True
9     group0.block0.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
10    group0.block0.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
11    group0.block0.convdim             (32, 16, 1, 1)          torch.cuda.FloatTensor True
12    group0.block1.conv0               (32, 32, 3, 3)          torch.cuda.FloatTensor True
13    group0.block1.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor True
14    group0.block1.bn0.weight          (32,)                   torch.cuda.FloatTensor True
15    group0.block1.bn0.bias            (32,)                   torch.cuda.FloatTensor True
16    group0.block1.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
17    group0.block1.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
18    group0.block1.bn1.weight          (32,)                   torch.cuda.FloatTensor True
19    group0.block1.bn1.bias            (32,)                   torch.cuda.FloatTensor True
20    group0.block1.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
21    group0.block1.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
22    group1.block0.conv0               (64, 32, 3, 3)          torch.cuda.FloatTensor True
23    group1.block0.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor True
24    group1.block0.bn0.weight          (32,)                   torch.cuda.FloatTensor True
25    group1.block0.bn0.bias            (32,)                   torch.cuda.FloatTensor True
26    group1.block0.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
27    group1.block0.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
28    group1.block0.bn1.weight          (64,)                   torch.cuda.FloatTensor True
29    group1.block0.bn1.bias            (64,)                   torch.cuda.FloatTensor True
30    group1.block0.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
31    group1.block0.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
32    group1.block0.convdim             (64, 32, 1, 1)          torch.cuda.FloatTensor True
33    group1.block1.conv0               (64, 64, 3, 3)          torch.cuda.FloatTensor True
34    group1.block1.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor True
35    group1.block1.bn0.weight          (64,)                   torch.cuda.FloatTensor True
36    group1.block1.bn0.bias            (64,)                   torch.cuda.FloatTensor True
37    group1.block1.bn0.running_mean    (64,)                   torch.cuda.FloatTensor False
38    group1.block1.bn0.running_var     (64,)                   torch.cuda.FloatTensor False
39    group1.block1.bn1.weight          (64,)                   torch.cuda.FloatTensor True
40    group1.block1.bn1.bias            (64,)                   torch.cuda.FloatTensor True
41    group1.block1.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
42    group1.block1.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
43    group2.block0.conv0               (128, 64, 3, 3)         torch.cuda.FloatTensor True
44    group2.block0.conv1               (128, 128, 3, 3)        torch.cuda.FloatTensor True
45    group2.block0.bn0.weight          (64,)                   torch.cuda.FloatTensor True
46    group2.block0.bn0.bias            (64,)                   torch.cuda.FloatTensor True
47    group2.block0.bn0.running_mean    (64,)                   torch.cuda.FloatTensor False
48    group2.block0.bn0.running_var     (64,)                   torch.cuda.FloatTensor False
49    group2.block0.bn1.weight          (128,)                  torch.cuda.FloatTensor True
50    group2.block0.bn1.bias            (128,)                  torch.cuda.FloatTensor True
51    group2.block0.bn1.running_mean    (128,)                  torch.cuda.FloatTensor False
52    group2.block0.bn1.running_var     (128,)                  torch.cuda.FloatTensor False
53    group2.block0.convdim             (128, 64, 1, 1)         torch.cuda.FloatTensor True
54    group2.block1.conv0               (128, 128, 3, 3)        torch.cuda.FloatTensor True
55    group2.block1.conv1               (128, 128, 3, 3)        torch.cuda.FloatTensor True
56    group2.block1.bn0.weight          (128,)                  torch.cuda.FloatTensor True
57    group2.block1.bn0.bias            (128,)                  torch.cuda.FloatTensor True
58    group2.block1.bn0.running_mean    (128,)                  torch.cuda.FloatTensor False
59    group2.block1.bn0.running_var     (128,)                  torch.cuda.FloatTensor False
60    group2.block1.bn1.weight          (128,)                  torch.cuda.FloatTensor True
61    group2.block1.bn1.bias            (128,)                  torch.cuda.FloatTensor True
62    group2.block1.bn1.running_mean    (128,)                  torch.cuda.FloatTensor False
63    group2.block1.bn1.running_var     (128,)                  torch.cuda.FloatTensor False
64    bn.weight                         (128,)                  torch.cuda.FloatTensor True
65    bn.bias                           (128,)                  torch.cuda.FloatTensor True
66    bn.running_mean                   (128,)                  torch.cuda.FloatTensor False
67    bn.running_var                    (128,)                  torch.cuda.FloatTensor False
68    fc.weight                         (10, 128)               torch.cuda.FloatTensor True
69    fc.bias                           (10,)                   torch.cuda.FloatTensor True

Total number of parameters: 693498
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:12<00:00,  7.93it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.692374808447701, 'train_acc': 36.946, 'test_loss': 1.6438849985599517, 'test_acc': 42.0699999999
9999, 'epoch': 1, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 12.359671115875244, 'test_time': 2.409191846847534, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (1/10), test_acc: 42.07
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.70it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.1892163017574622, 'train_acc': 56.99799999999999, 'test_loss': 1.1359874844551088, 'test_acc': 5
8.529999999999994, 'epoch': 2, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.099769115447998, 'test_time': 2.3490748405456543, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (2/10), test_acc: 58.53
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.67it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.9585816251988314, 'train_acc': 65.67399999999999, 'test_loss': 1.0047420024871827, 'test_acc': 6
4.13, 'epoch': 3, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.134236097335815, 'test_time': 2.2976908683776855, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (3/10), test_acc: 64.13
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:09<00:00,  9.80it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.8303685279525059, 'train_acc': 70.502, 'test_loss': 1.2769802391529081, 'test_acc': 57.16, 'epoc
h': 4, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 9.999990224838257, 'test_time': 2.276317596435547, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (4/10), test_acc: 57.16
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.79it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.7388625071973219, 'train_acc': 74.088, 'test_loss': 0.8904295802116394, 'test_acc': 69.25, 'epoc
h': 5, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.005611896514893, 'test_time': 2.2940855026245117, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (5/10), test_acc: 69.25
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:09<00:00,  9.81it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.6575297518652317, 'train_acc': 77.01400000000001, 'test_loss': 0.9554895848035814, 'test_acc': 6
7.07000000000001, 'epoch': 6, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 9.993067979812622, 'test_time': 2.280683755874634, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (6/10), test_acc: 67.07
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.72it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.6062442544771703, 'train_acc': 78.908, 'test_loss': 0.8814762473106382, 'test_acc': 71.56, 'epoc
h': 7, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.07930040359497, 'test_time': 2.2808926105499268, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (7/10), test_acc: 71.56
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.78it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.5559366880630958, 'train_acc': 80.76, 'test_loss': 0.7340627878904343, 'test_acc': 75.3500000000
0001, 'epoch': 8, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.022404432296753, 'test_time': 2.283658504486084, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
==> id: logs/resnet_16_2_teacher (8/10), test_acc: 75.35
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.71it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.5228431063647176, 'train_acc': 81.82199999999999, 'test_loss': 1.1339433610439302, 'test_acc': 6
6.82000000000001, 'epoch': 9, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.096271514892578, 'test_time': 2.3363120555877686, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (9/10), test_acc: 66.82
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.71it/s] 
{'depth': 16, 'width': 2.0, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': '', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '',
 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 0, 'cuda': True, 'save': 'logs/resnet_16_2_teacher', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 0.4926957174831506, 'train_acc': 83.06, 'test_loss': 0.7887022227048874, 'test_acc': 74.31, 'epoch
': 10, 'num_classes': 10, 'n_parameters': 693498, 'train_time': 10.091069221496582, 'test_time': 2.3068747520446777, 'at_losses': [(nan, nan), (nan, nan), (nan, nan)]}
None
==> id: logs/resnet_16_2_teacher (10/10), test_acc: 74.31
PS D:\desk\yan\code\attention-transfer> python cifar.py --save logs/at_16_1_16_2 --teacher_id resnet_16_2_teacher --beta 1e+3 --cuda
parsed options: {'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr
_decay_ratio': 0.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0'}
Files already downloaded and verified
Files already downloaded and verified
creating optimizer with lr =  0.1

Parameters:
0     student.conv0                             (16, 3, 3, 3)           torch.cuda.FloatTensor True
1     student.group0.block0.conv0               (16, 16, 3, 3)          torch.cuda.FloatTensor True
2     student.group0.block0.conv1               (16, 16, 3, 3)          torch.cuda.FloatTensor True
3     student.group0.block0.bn0.weight          (16,)                   torch.cuda.FloatTensor True
4     student.group0.block0.bn0.bias            (16,)                   torch.cuda.FloatTensor True
5     student.group0.block0.bn0.running_mean    (16,)                   torch.cuda.FloatTensor False
6     student.group0.block0.bn0.running_var     (16,)                   torch.cuda.FloatTensor False
7     student.group0.block0.bn1.weight          (16,)                   torch.cuda.FloatTensor True
8     student.group0.block0.bn1.bias            (16,)                   torch.cuda.FloatTensor True
9     student.group0.block0.bn1.running_mean    (16,)                   torch.cuda.FloatTensor False
10    student.group0.block0.bn1.running_var     (16,)                   torch.cuda.FloatTensor False
11    student.group0.block1.conv0               (16, 16, 3, 3)          torch.cuda.FloatTensor True
12    student.group0.block1.conv1               (16, 16, 3, 3)          torch.cuda.FloatTensor True
13    student.group0.block1.bn0.weight          (16,)                   torch.cuda.FloatTensor True
14    student.group0.block1.bn0.bias            (16,)                   torch.cuda.FloatTensor True
15    student.group0.block1.bn0.running_mean    (16,)                   torch.cuda.FloatTensor False
16    student.group0.block1.bn0.running_var     (16,)                   torch.cuda.FloatTensor False
17    student.group0.block1.bn1.weight          (16,)                   torch.cuda.FloatTensor True
18    student.group0.block1.bn1.bias            (16,)                   torch.cuda.FloatTensor True
19    student.group0.block1.bn1.running_mean    (16,)                   torch.cuda.FloatTensor False
20    student.group0.block1.bn1.running_var     (16,)                   torch.cuda.FloatTensor False
21    student.group1.block0.conv0               (32, 16, 3, 3)          torch.cuda.FloatTensor True
22    student.group1.block0.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor True
23    student.group1.block0.bn0.weight          (16,)                   torch.cuda.FloatTensor True
24    student.group1.block0.bn0.bias            (16,)                   torch.cuda.FloatTensor True
25    student.group1.block0.bn0.running_mean    (16,)                   torch.cuda.FloatTensor False
26    student.group1.block0.bn0.running_var     (16,)                   torch.cuda.FloatTensor False
27    student.group1.block0.bn1.weight          (32,)                   torch.cuda.FloatTensor True
28    student.group1.block0.bn1.bias            (32,)                   torch.cuda.FloatTensor True
29    student.group1.block0.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
30    student.group1.block0.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
31    student.group1.block0.convdim             (32, 16, 1, 1)          torch.cuda.FloatTensor True
32    student.group1.block1.conv0               (32, 32, 3, 3)          torch.cuda.FloatTensor True
33    student.group1.block1.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor True
34    student.group1.block1.bn0.weight          (32,)                   torch.cuda.FloatTensor True
35    student.group1.block1.bn0.bias            (32,)                   torch.cuda.FloatTensor True
36    student.group1.block1.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
37    student.group1.block1.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
38    student.group1.block1.bn1.weight          (32,)                   torch.cuda.FloatTensor True
39    student.group1.block1.bn1.bias            (32,)                   torch.cuda.FloatTensor True
40    student.group1.block1.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
41    student.group1.block1.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
42    student.group2.block0.conv0               (64, 32, 3, 3)          torch.cuda.FloatTensor True
43    student.group2.block0.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor True
44    student.group2.block0.bn0.weight          (32,)                   torch.cuda.FloatTensor True
45    student.group2.block0.bn0.bias            (32,)                   torch.cuda.FloatTensor True
46    student.group2.block0.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
47    student.group2.block0.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
48    student.group2.block0.bn1.weight          (64,)                   torch.cuda.FloatTensor True
49    student.group2.block0.bn1.bias            (64,)                   torch.cuda.FloatTensor True
50    student.group2.block0.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
51    student.group2.block0.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
52    student.group2.block0.convdim             (64, 32, 1, 1)          torch.cuda.FloatTensor True
53    student.group2.block1.conv0               (64, 64, 3, 3)          torch.cuda.FloatTensor True
54    student.group2.block1.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor True
55    student.group2.block1.bn0.weight          (64,)                   torch.cuda.FloatTensor True
56    student.group2.block1.bn0.bias            (64,)                   torch.cuda.FloatTensor True
57    student.group2.block1.bn0.running_mean    (64,)                   torch.cuda.FloatTensor False
58    student.group2.block1.bn0.running_var     (64,)                   torch.cuda.FloatTensor False
59    student.group2.block1.bn1.weight          (64,)                   torch.cuda.FloatTensor True
60    student.group2.block1.bn1.bias            (64,)                   torch.cuda.FloatTensor True
61    student.group2.block1.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
62    student.group2.block1.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
63    student.bn.weight                         (64,)                   torch.cuda.FloatTensor True
64    student.bn.bias                           (64,)                   torch.cuda.FloatTensor True
65    student.bn.running_mean                   (64,)                   torch.cuda.FloatTensor False
66    student.bn.running_var                    (64,)                   torch.cuda.FloatTensor False
67    student.fc.weight                         (10, 64)                torch.cuda.FloatTensor True
68    student.fc.bias                           (10,)                   torch.cuda.FloatTensor True
69    teacher.conv0                             (16, 3, 3, 3)           torch.cuda.FloatTensor False
70    teacher.group0.block0.conv0               (32, 16, 3, 3)          torch.cuda.FloatTensor False
71    teacher.group0.block0.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor False
72    teacher.group0.block0.bn0.weight          (16,)                   torch.cuda.FloatTensor False
73    teacher.group0.block0.bn0.bias            (16,)                   torch.cuda.FloatTensor False
74    teacher.group0.block0.bn0.running_mean    (16,)                   torch.cuda.FloatTensor False
75    teacher.group0.block0.bn0.running_var     (16,)                   torch.cuda.FloatTensor False
76    teacher.group0.block0.bn1.weight          (32,)                   torch.cuda.FloatTensor False
77    teacher.group0.block0.bn1.bias            (32,)                   torch.cuda.FloatTensor False
78    teacher.group0.block0.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
79    teacher.group0.block0.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
80    teacher.group0.block0.convdim             (32, 16, 1, 1)          torch.cuda.FloatTensor False
81    teacher.group0.block1.conv0               (32, 32, 3, 3)          torch.cuda.FloatTensor False
82    teacher.group0.block1.conv1               (32, 32, 3, 3)          torch.cuda.FloatTensor False
83    teacher.group0.block1.bn0.weight          (32,)                   torch.cuda.FloatTensor False
84    teacher.group0.block1.bn0.bias            (32,)                   torch.cuda.FloatTensor False
85    teacher.group0.block1.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
86    teacher.group0.block1.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
87    teacher.group0.block1.bn1.weight          (32,)                   torch.cuda.FloatTensor False
88    teacher.group0.block1.bn1.bias            (32,)                   torch.cuda.FloatTensor False
89    teacher.group0.block1.bn1.running_mean    (32,)                   torch.cuda.FloatTensor False
90    teacher.group0.block1.bn1.running_var     (32,)                   torch.cuda.FloatTensor False
91    teacher.group1.block0.conv0               (64, 32, 3, 3)          torch.cuda.FloatTensor False
92    teacher.group1.block0.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor False
93    teacher.group1.block0.bn0.weight          (32,)                   torch.cuda.FloatTensor False
94    teacher.group1.block0.bn0.bias            (32,)                   torch.cuda.FloatTensor False
95    teacher.group1.block0.bn0.running_mean    (32,)                   torch.cuda.FloatTensor False
96    teacher.group1.block0.bn0.running_var     (32,)                   torch.cuda.FloatTensor False
97    teacher.group1.block0.bn1.weight          (64,)                   torch.cuda.FloatTensor False
98    teacher.group1.block0.bn1.bias            (64,)                   torch.cuda.FloatTensor False
99    teacher.group1.block0.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
100   teacher.group1.block0.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
101   teacher.group1.block0.convdim             (64, 32, 1, 1)          torch.cuda.FloatTensor False
102   teacher.group1.block1.conv0               (64, 64, 3, 3)          torch.cuda.FloatTensor False
103   teacher.group1.block1.conv1               (64, 64, 3, 3)          torch.cuda.FloatTensor False
104   teacher.group1.block1.bn0.weight          (64,)                   torch.cuda.FloatTensor False
105   teacher.group1.block1.bn0.bias            (64,)                   torch.cuda.FloatTensor False
106   teacher.group1.block1.bn0.running_mean    (64,)                   torch.cuda.FloatTensor False
107   teacher.group1.block1.bn0.running_var     (64,)                   torch.cuda.FloatTensor False
108   teacher.group1.block1.bn1.weight          (64,)                   torch.cuda.FloatTensor False
109   teacher.group1.block1.bn1.bias            (64,)                   torch.cuda.FloatTensor False
110   teacher.group1.block1.bn1.running_mean    (64,)                   torch.cuda.FloatTensor False
111   teacher.group1.block1.bn1.running_var     (64,)                   torch.cuda.FloatTensor False
112   teacher.group2.block0.conv0               (128, 64, 3, 3)         torch.cuda.FloatTensor False
113   teacher.group2.block0.conv1               (128, 128, 3, 3)        torch.cuda.FloatTensor False
114   teacher.group2.block0.bn0.weight          (64,)                   torch.cuda.FloatTensor False
115   teacher.group2.block0.bn0.bias            (64,)                   torch.cuda.FloatTensor False
116   teacher.group2.block0.bn0.running_mean    (64,)                   torch.cuda.FloatTensor False
117   teacher.group2.block0.bn0.running_var     (64,)                   torch.cuda.FloatTensor False
118   teacher.group2.block0.bn1.weight          (128,)                  torch.cuda.FloatTensor False
119   teacher.group2.block0.bn1.bias            (128,)                  torch.cuda.FloatTensor False
120   teacher.group2.block0.bn1.running_mean    (128,)                  torch.cuda.FloatTensor False
121   teacher.group2.block0.bn1.running_var     (128,)                  torch.cuda.FloatTensor False
122   teacher.group2.block0.convdim             (128, 64, 1, 1)         torch.cuda.FloatTensor False
123   teacher.group2.block1.conv0               (128, 128, 3, 3)        torch.cuda.FloatTensor False
124   teacher.group2.block1.conv1               (128, 128, 3, 3)        torch.cuda.FloatTensor False
125   teacher.group2.block1.bn0.weight          (128,)                  torch.cuda.FloatTensor False
126   teacher.group2.block1.bn0.bias            (128,)                  torch.cuda.FloatTensor False
127   teacher.group2.block1.bn0.running_mean    (128,)                  torch.cuda.FloatTensor False
128   teacher.group2.block1.bn0.running_var     (128,)                  torch.cuda.FloatTensor False
129   teacher.group2.block1.bn1.weight          (128,)                  torch.cuda.FloatTensor False
130   teacher.group2.block1.bn1.bias            (128,)                  torch.cuda.FloatTensor False
131   teacher.group2.block1.bn1.running_mean    (128,)                  torch.cuda.FloatTensor False
132   teacher.group2.block1.bn1.running_var     (128,)                  torch.cuda.FloatTensor False
133   teacher.bn.weight                         (128,)                  torch.cuda.FloatTensor False
134   teacher.bn.bias                           (128,)                  torch.cuda.FloatTensor False
135   teacher.bn.running_mean                   (128,)                  torch.cuda.FloatTensor False
136   teacher.bn.running_var                    (128,)                  torch.cuda.FloatTensor False
137   teacher.fc.weight                         (10, 128)               torch.cuda.FloatTensor False
138   teacher.fc.bias                           (10,)                   torch.cuda.FloatTensor False

Total number of parameters: 175994
  0%|                                                                                                                                                                                                                              | 0/98 [00:00<?, ?it/s]D
:\desk\yan\code\attention-transfer\venv\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:11<00:00,  8.23it/s]
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 3.8131965423116885, 'train_acc': 31.774, 'test_loss': 3.0002587199211117, 'test_acc
': 42.65, 'epoch': 1, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 11.912145137786865, 'test_time': 2.475377321243286, 'at_losses': [(0.0001413140793282132, 0.00014358886671257562), (0.0003360679259196205, 0.0001258019369046006), (0.001415
844864638175, 0.0005647013627208027)]}
None
==> id: logs/at_16_1_16_2 (1/10), test_acc: 42.65
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.74it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 2.5138492511243227, 'train_acc': 54.964, 'test_loss': 2.4479469060897823, 'test_acc
': 55.32, 'epoch': 2, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.05871033668518, 'test_time': 2.302677869796753, 'at_losses': [(4.873190572020652e-05, 8.068909625477126e-06), (0.00021790084934702648, 2.1678218687437406e-05), (0.000988
2159790584534, 6.844322509153173e-05)]}
None
==> id: logs/at_16_1_16_2 (2/10), test_acc: 55.32
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.78it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.9661938085847974, 'train_acc': 64.504, 'test_loss': 1.8868767797946928, 'test_acc
': 65.7, 'epoch': 3, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.024161338806152, 'test_time': 2.3871712684631348, 'at_losses': [(3.2472130503989875e-05, 3.884621064194534e-06), (0.00016511237678923092, 1.3477666741070962e-05), (0.0007
762987348172119, 4.924653697935897e-05)]}
None
==> id: logs/at_16_1_16_2 (3/10), test_acc: 65.70
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:09<00:00,  9.81it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.6340487745343422, 'train_acc': 70.712, 'test_loss': 1.6405271530151369, 'test_acc
': 70.52000000000001, 'epoch': 4, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 9.986584186553955, 'test_time': 2.4133169651031494, 'at_losses': [(2.489951689852405e-05, 2.151177667957636e-06), (0.00013459223709552608, 8.581973028887108e-06
), (0.0006453565946361841, 3.316530663291537e-05)]}
None
==> id: logs/at_16_1_16_2 (4/10), test_acc: 70.52
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.78it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.4272853476660594, 'train_acc': 74.536, 'test_loss': 1.5569196105003358, 'test_acc
': 72.48, 'epoch': 5, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.017881155014038, 'test_time': 2.3141047954559326, 'at_losses': [(2.1563764013250374e-05, 1.0462601245916883e-06), (0.0001145718921734596, 6.2807320753640915e-06), (0.000
5745657821288487, 3.171863370760445e-05)]}
None
==> id: logs/at_16_1_16_2 (5/10), test_acc: 72.48
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:09<00:00,  9.87it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.2889908430527668, 'train_acc': 76.558, 'test_loss': 1.5050851225852968, 'test_acc
': 72.76, 'epoch': 6, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 9.929941177368164, 'test_time': 2.3433618545532227, 'at_losses': [(1.9165933289036355e-05, 8.163825334863999e-07), (0.0001013198567399463, 4.093568821531943e-06), (0.000515
7559316396178, 3.052027133463625e-05)]}
None
==> id: logs/at_16_1_16_2 (6/10), test_acc: 72.76
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.73it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.1933907221774671, 'train_acc': 78.374, 'test_loss': 1.4939655244350432, 'test_acc
': 74.32000000000001, 'epoch': 7, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.071029424667358, 'test_time': 2.365178108215332, 'at_losses': [(1.7995829894370204e-05, 5.891085324773237e-07), (9.194881839271229e-05, 3.214492890546821e-06
), (0.000489393450167835, 5.0947338186358846e-05)]}
None
==> id: logs/at_16_1_16_2 (7/10), test_acc: 74.32
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.59it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.1324002973887388, 'train_acc': 79.5, 'test_loss': 1.2145948708057406, 'test_acc':
 77.47999999999999, 'epoch': 8, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.222965717315674, 'test_time': 2.3681538105010986, 'at_losses': [(1.6642890178615236e-05, 6.318642345934902e-07), (8.47211301596085e-05, 2.377572186255206e-06),
 (0.00044640067124234194, 1.721351665790093e-05)]}
None
==> id: logs/at_16_1_16_2 (8/10), test_acc: 77.48
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:09<00:00,  9.86it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.0639551135958456, 'train_acc': 80.81, 'test_loss': 1.1376338720321655, 'test_acc'
: 79.91, 'epoch': 9, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 9.944695949554443, 'test_time': 2.354954719543457, 'at_losses': [(1.5437153336479535e-05, 4.6323805186038937e-07), (7.978889167997437e-05, 2.194542276058727e-06), (0.0004225
203343686821, 2.1586254514443795e-05)]}
None
==> id: logs/at_16_1_16_2 (9/10), test_acc: 79.91
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:10<00:00,  9.80it/s] 
{'depth': 16, 'width': 1, 'dataset': 'CIFAR10', 'dataroot': '.', 'dtype': 'float', 'nthread': 2, 'teacher_id': 'resnet_16_2_teacher', 'batch_size': 512, 'lr': 0.1, 'epochs': 10, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0
.2, 'resume': '', 'randomcrop_pad': 4, 'temperature': 4, 'alpha': 0, 'beta': 1000.0, 'cuda': True, 'save': 'logs/at_16_1_16_2', 'ngpu': 1, 'gpu_id': '0', 'train_loss': 1.0336430656666653, 'train_acc': 81.28999999999999, 'test_loss': 1.179079085588455,
 'test_acc': 78.3, 'epoch': 10, 'num_classes': 10, 'n_parameters': 175994, 'train_time': 10.0033700466156, 'test_time': 2.307983636856079, 'at_losses': [(1.5011511696699873e-05, 5.465170203684692e-07), (7.754037764921815e-05, 2.6721522228024093e-06), 
(0.00041453409431920526, 2.227595950557106e-05)]}
None
==> id: logs/at_16_1_16_2 (10/10), test_acc: 78.30







{'network': <function main.<locals>.h at 0x00000181E633B8B8>, 'iterator': <tqdm.std.tqdm object at 0x00000181E631FE08>, 'maxepoch': 10, 'optimizer': SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
), 'epoch': 0, 't': 34, 'train': True, 'sample': [tensor([[[[-1.8143e+00, -1.7667e+00, -1.6397e+00,  ..., -1.7508e+00,
           -1.5921e+00, -1.7508e+00],
          [-1.7190e+00, -1.7667e+00, -1.7508e+00,  ..., -1.3222e+00,
           -1.4175e+00, -1.3222e+00],
          [-1.7508e+00, -1.7667e+00, -1.8143e+00,  ..., -3.6508e-02,
           -3.6508e-02, -3.6508e-02],
          ...,
          [-4.4921e-01, -7.3492e-01, -1.5127e+00,  ...,  2.6508e-01,
           -2.0635e-02,  2.6508e-01],
          [ 5.8730e-02,  1.2222e-01, -3.5397e-01,  ...,  9.4762e-01,
            9.6349e-01,  9.4762e-01],
          [ 9.3175e-01,  8.6825e-01,  9.6349e-01,  ...,  9.9524e-01,
            1.0905e+00,  9.9524e-01]],

         [[-1.5781e+00, -1.5137e+00, -1.4010e+00,  ..., -1.4976e+00,
           -1.3849e+00, -1.4976e+00],
          [-1.5298e+00, -1.5781e+00, -1.5459e+00,  ..., -1.0306e+00,
           -1.1916e+00, -1.0306e+00],
          [-1.6103e+00, -1.6425e+00, -1.6586e+00,  ..., -2.8986e-01,
           -3.3816e-01, -2.8986e-01],
          ...,
          [-1.9324e-01, -4.8309e-01, -1.3849e+00,  ...,  3.2206e-01,
            3.2206e-02,  3.2206e-01],
          [ 1.2882e-01,  2.0934e-01, -3.2206e-01,  ...,  9.0177e-01,
            9.1787e-01,  9.0177e-01],
          [ 9.1787e-01,  8.8567e-01,  9.6618e-01,  ...,  9.6618e-01,
            1.0467e+00,  9.6618e-01]],

         [[-1.5127e+00, -1.4678e+00, -1.3628e+00,  ..., -1.4528e+00,
           -1.3028e+00, -1.4528e+00],
          [-1.4078e+00, -1.4528e+00, -1.4528e+00,  ..., -1.1979e+00,
           -1.2579e+00, -1.1979e+00],
          [-1.4378e+00, -1.4528e+00, -1.5127e+00,  ..., -7.1814e-01,
           -7.7811e-01, -7.1814e-01],
          ...,
          [-6.2819e-01, -8.3808e-01, -1.4228e+00,  ..., -1.3343e-01,
           -3.5832e-01, -1.3343e-01],
          [-1.0345e-01,  1.4993e-03, -4.0330e-01,  ...,  7.2114e-01,
            7.0615e-01,  7.2114e-01],
          [ 8.2609e-01,  7.8111e-01,  8.4108e-01,  ...,  8.5607e-01,
            9.1604e-01,  8.5607e-01]]],


        [[[-1.3540e+00, -1.1476e+00, -7.0317e-01,  ..., -1.2429e+00,
           -1.1159e+00, -1.0524e+00],
          [-1.3381e+00, -1.0206e+00, -8.1429e-01,  ..., -1.5603e+00,
           -1.5603e+00, -1.4333e+00],
          [-1.3540e+00, -1.1476e+00, -7.0317e-01,  ..., -1.2429e+00,
           -1.1159e+00, -1.0524e+00],
          ...,
          [-1.0048e+00, -9.7302e-01, -9.5714e-01,  ..., -1.1476e+00,
           -1.1159e+00, -1.1317e+00],
          [-9.5714e-01, -8.9365e-01, -8.7778e-01,  ..., -1.1000e+00,
           -1.0841e+00, -1.0683e+00],
          [-8.9365e-01, -8.3016e-01, -8.3016e-01,  ..., -1.0524e+00,
           -1.0524e+00, -1.0524e+00]],

         [[-1.6264e+00, -1.0950e+00, -4.6699e-01,  ..., -1.1594e+00,
           -1.0306e+00, -9.8229e-01],
          [-1.5298e+00, -9.1787e-01, -5.9581e-01,  ..., -1.4654e+00,
           -1.4493e+00, -1.3205e+00],
          [-1.6264e+00, -1.0950e+00, -4.6699e-01,  ..., -1.1594e+00,
           -1.0306e+00, -9.8229e-01],
          ...,
          [-1.4976e+00, -1.4654e+00, -1.4332e+00,  ..., -1.6264e+00,
           -1.5781e+00, -1.5942e+00],
          [-1.4976e+00, -1.4332e+00, -1.4171e+00,  ..., -1.5781e+00,
           -1.5620e+00, -1.5459e+00],
          [-1.4332e+00, -1.3849e+00, -1.4010e+00,  ..., -1.5459e+00,
           -1.5459e+00, -1.5459e+00]],

         [[-1.2279e+00, -8.2309e-01, -1.9340e-01,  ..., -9.7301e-01,
           -8.6807e-01, -8.0810e-01],
          [-1.1379e+00, -6.5817e-01, -3.4333e-01,  ..., -1.2429e+00,
           -1.2429e+00, -1.1079e+00],
          [-1.2279e+00, -8.2309e-01, -1.9340e-01,  ..., -9.7301e-01,
           -8.6807e-01, -8.0810e-01],
          ...,
          [-1.1679e+00, -1.1379e+00, -1.1079e+00,  ..., -1.2579e+00,
           -1.2129e+00, -1.2279e+00],
          [-1.1529e+00, -1.1229e+00, -1.0930e+00,  ..., -1.2129e+00,
           -1.1979e+00, -1.1829e+00],
          [-1.0780e+00, -1.0480e+00, -1.0330e+00,  ..., -1.1679e+00,
           -1.1829e+00, -1.1679e+00]]],


        [[[-1.0048e+00, -8.1429e-01, -1.0206e+00,  ...,  1.0587e+00,
            3.9206e-01,  1.0587e+00],
          [-9.4127e-01, -6.2381e-01, -7.1905e-01,  ...,  1.5397e-01,
            6.7778e-01,  1.5397e-01],
          [-9.7302e-01, -3.3810e-01, -5.4444e-01,  ...,  1.0635e-01,
            5.8254e-01,  1.0635e-01],
          ...,
          [-5.2857e-01, -6.8730e-01, -7.1905e-01,  ..., -7.5079e-01,
           -1.1159e+00, -7.5079e-01],
          [-5.9206e-01, -5.7619e-01, -8.4603e-01,  ..., -1.1317e+00,
           -1.3698e+00, -1.1317e+00],
          [-7.5079e-01, -5.2857e-01, -1.0683e+00,  ..., -1.1159e+00,
           -1.2270e+00, -1.1159e+00]],

         [[-1.1272e+00, -8.8567e-01, -1.0467e+00,  ...,  9.0177e-01,
            3.0596e-01,  9.0177e-01],
          [-1.0628e+00, -7.2464e-01, -7.7295e-01,  ...,  3.2206e-02,
            6.2802e-01,  3.2206e-02],
          [-1.1272e+00, -4.5089e-01, -6.1192e-01,  ...,  1.1272e-01,
            5.4750e-01,  1.1272e-01],
          ...,
          [-4.0258e-01, -8.0515e-01, -8.6957e-01,  ..., -8.3736e-01,
           -1.1433e+00, -8.3736e-01],
          [-5.1530e-01, -7.4074e-01, -1.0789e+00,  ..., -1.2560e+00,
           -1.4654e+00, -1.2560e+00],
          [-6.7633e-01, -8.0515e-01, -1.2399e+00,  ..., -1.2721e+00,
           -1.3849e+00, -1.2721e+00]],

         [[-1.0330e+00, -8.6807e-01, -1.0630e+00,  ...,  2.2639e-01,
           -4.3328e-01,  2.2639e-01],
          [-9.8801e-01, -7.1814e-01, -8.2309e-01,  ..., -6.7316e-01,
           -3.1334e-01, -6.7316e-01],
          [-1.0480e+00, -4.6327e-01, -6.5817e-01,  ..., -4.9325e-01,
           -2.5337e-01, -4.9325e-01],
          ...,
          [-5.0825e-01, -7.9310e-01, -8.9805e-01,  ..., -1.0180e+00,
           -1.1079e+00, -1.0180e+00],
          [-6.8816e-01, -8.3808e-01, -1.0780e+00,  ..., -1.1229e+00,
           -1.2429e+00, -1.1229e+00],
          [-9.1304e-01, -9.2804e-01, -1.1679e+00,  ..., -1.0930e+00,
           -1.1979e+00, -1.0930e+00]]],


        ...,


        [[[ 6.1429e-01,  6.9365e-01,  6.4603e-01,  ...,  5.5079e-01,
            6.3016e-01,  6.6190e-01],
          [ 7.5714e-01,  7.2540e-01,  7.0952e-01,  ...,  6.3016e-01,
            6.7778e-01,  7.2540e-01],
          [ 6.1429e-01,  6.9365e-01,  6.4603e-01,  ...,  5.5079e-01,
            6.3016e-01,  6.6190e-01],
          ...,
          [ 7.0952e-01,  5.8254e-01,  4.8730e-01,  ..., -4.9683e-01,
           -5.1270e-01, -1.2270e+00],
          [ 6.4603e-01,  5.9841e-01,  4.3968e-01,  ..., -3.0635e-01,
           -5.4444e-01, -1.2905e+00],
          [ 5.5079e-01,  5.9841e-01,  5.6667e-01,  ..., -1.7937e-01,
           -6.7143e-01, -1.0524e+00]],

         [[ 6.7633e-01,  7.2464e-01,  6.6023e-01,  ...,  6.2802e-01,
            7.0853e-01,  7.4074e-01],
          [ 8.0515e-01,  7.7295e-01,  7.5684e-01,  ...,  6.9243e-01,
            7.5684e-01,  8.0515e-01],
          [ 6.7633e-01,  7.2464e-01,  6.6023e-01,  ...,  6.2802e-01,
            7.0853e-01,  7.4074e-01],
          ...,
          [ 7.4074e-01,  6.2802e-01,  4.9919e-01,  ..., -1.1755e+00,
           -1.1916e+00, -1.3688e+00],
          [ 7.0853e-01,  6.1192e-01,  4.1868e-01,  ..., -9.8229e-01,
           -1.1272e+00, -1.3849e+00],
          [ 6.1192e-01,  6.6023e-01,  6.1192e-01,  ..., -9.6618e-01,
           -1.2238e+00, -1.3527e+00]],

         [[ 5.7121e-01,  6.1619e-01,  5.5622e-01,  ...,  5.5622e-01,
            6.1619e-01,  6.4618e-01],
          [ 7.0615e-01,  6.7616e-01,  6.6117e-01,  ...,  6.6117e-01,
            6.9115e-01,  7.5112e-01],
          [ 5.7121e-01,  6.1619e-01,  5.5622e-01,  ...,  5.5622e-01,
            6.1619e-01,  6.4618e-01],
          ...,
          [ 6.3118e-01,  4.8126e-01,  3.4633e-01,  ..., -1.1829e+00,
           -1.2129e+00, -1.2579e+00],
          [ 6.0120e-01,  4.8126e-01,  2.5637e-01,  ..., -1.1079e+00,
           -1.1829e+00, -1.2579e+00],
          [ 5.1124e-01,  5.4123e-01,  4.8126e-01,  ..., -1.1379e+00,
           -1.1979e+00, -1.2429e+00]]],


        [[[ 5.8730e-02, -3.6984e-01,  2.6508e-01,  ..., -4.9683e-01,
            2.4921e-01, -3.6508e-02],
          [ 2.6984e-02,  1.2222e-01,  7.4603e-02,  ..., -2.2698e-01,
            7.4603e-02,  5.8730e-02],
          [-1.1587e-01, -3.6508e-02, -1.3175e-01,  ..., -4.0159e-01,
           -3.6508e-02,  4.7143e-01],
          ...,
          [ 2.1746e-01,  1.1111e-02, -4.7619e-03,  ..., -3.5397e-01,
           -2.2698e-01, -1.0000e-01],
          [ 2.4921e-01, -2.0635e-02, -6.2381e-01,  ...,  2.6984e-02,
           -4.7619e-03,  1.0635e-01],
          [ 1.0635e-01, -8.4127e-02, -2.5873e-01,  ...,  9.0476e-02,
            2.6508e-01,  4.2381e-01]],

         [[-1.6103e-02, -4.6699e-01,  1.7713e-01,  ..., -7.4074e-01,
            1.2882e-01, -1.4493e-01],
          [-3.2206e-02,  4.8309e-02,  1.6103e-02,  ..., -3.8647e-01,
           -3.2206e-02, -3.2206e-02],
          [-1.6103e-01, -9.6618e-02, -1.7713e-01,  ..., -4.9919e-01,
           -1.2882e-01,  4.0258e-01],
          ...,
          [ 2.0934e-01,  1.6103e-02,  0.0000e+00,  ..., -5.7971e-01,
           -2.4155e-01, -1.1272e-01],
          [ 2.4155e-01, -3.2206e-02, -6.6023e-01,  ..., -9.6618e-02,
           -1.6103e-02,  9.6618e-02],
          [ 6.4412e-02, -1.2882e-01, -3.2206e-01,  ...,  1.1272e-01,
            2.8986e-01,  4.1868e-01]],

         [[-8.2309e-01, -1.2279e+00, -6.4318e-01,  ..., -1.2729e+00,
           -6.8816e-01, -1.0180e+00],
          [-8.2309e-01, -7.4813e-01, -7.9310e-01,  ..., -1.0630e+00,
           -9.4303e-01, -9.8801e-01],
          [-9.2804e-01, -8.6807e-01, -9.7301e-01,  ..., -1.2729e+00,
           -1.1079e+00, -6.4318e-01],
          ...,
          [-6.5817e-01, -8.5307e-01, -9.2804e-01,  ..., -1.2129e+00,
           -1.1079e+00, -9.8801e-01],
          [-7.0315e-01, -9.2804e-01, -1.4678e+00,  ..., -9.4303e-01,
           -1.0030e+00, -8.8306e-01],
          [-9.5802e-01, -1.0930e+00, -1.2429e+00,  ..., -9.7301e-01,
           -8.2309e-01, -6.5817e-01]]],


        [[[-1.7032e+00, -1.7032e+00, -1.7032e+00,  ..., -1.4968e+00,
           -1.3222e+00, -1.5286e+00],
          [-1.4651e+00, -1.6873e+00, -1.4651e+00,  ..., -1.5603e+00,
           -1.4175e+00, -1.4492e+00],
          [-1.4333e+00, -1.5444e+00, -1.4333e+00,  ..., -1.5286e+00,
           -1.4492e+00, -1.4651e+00],
          ...,
          [-1.5603e+00, -1.5444e+00, -1.5603e+00,  ..., -8.4603e-01,
           -1.1476e+00, -1.4333e+00],
          [-1.4175e+00, -1.4810e+00, -1.4175e+00,  ..., -1.0048e+00,
           -1.0206e+00, -1.5127e+00],
          [-1.4175e+00, -1.6556e+00, -1.4175e+00,  ..., -1.1635e+00,
           -1.1000e+00, -1.3222e+00]],

         [[-1.3527e+00, -1.4332e+00, -1.3527e+00,  ..., -1.2560e+00,
           -1.1594e+00, -1.3366e+00],
          [-1.1433e+00, -1.3205e+00, -1.1433e+00,  ..., -1.3366e+00,
           -1.1272e+00, -1.1111e+00],
          [-1.0628e+00, -1.1272e+00, -1.0628e+00,  ..., -1.2077e+00,
           -1.1433e+00, -1.1594e+00],
          ...,
          [-9.9839e-01, -9.6618e-01, -9.9839e-01,  ...,  1.4493e-01,
           -2.5765e-01, -1.0467e+00],
          [-8.5346e-01, -9.1787e-01, -8.5346e-01,  ...,  0.0000e+00,
           -3.2206e-01, -1.2399e+00],
          [-8.0515e-01, -1.0145e+00, -8.0515e-01,  ..., -2.7375e-01,
           -5.7971e-01, -5.9581e-01]],

         [[-1.5277e+00, -1.5427e+00, -1.5277e+00,  ..., -1.5127e+00,
           -1.4228e+00, -1.4828e+00],
          [-1.4828e+00, -1.5577e+00, -1.4828e+00,  ..., -1.5277e+00,
           -1.4678e+00, -1.4678e+00],
          [-1.4828e+00, -1.4978e+00, -1.4828e+00,  ..., -1.4978e+00,
           -1.4978e+00, -1.5127e+00],
          ...,
          [-1.3628e+00, -1.3628e+00, -1.3628e+00,  ..., -5.5322e-01,
           -7.9310e-01, -1.1679e+00],
          [-1.2579e+00, -1.2729e+00, -1.2579e+00,  ..., -6.4318e-01,
           -6.5817e-01, -1.3178e+00],
          [-1.3028e+00, -1.3328e+00, -1.3028e+00,  ..., -8.5307e-01,
           -8.3808e-01, -1.0180e+00]]]]), tensor([7, 5, 4, 9, 2, 1, 5, 1, 8, 9, 7, 6, 6, 3, 5, 1, 8, 8, 9, 9, 1, 7, 7, 7,
        7, 7, 0, 0, 7, 4, 7, 5, 1, 5, 9, 6, 1, 1, 9, 1, 3, 7, 1, 4, 3, 7, 2, 9,
        8, 2, 9, 3, 5, 6, 3, 9, 1, 2, 4, 5, 1, 7, 8, 3, 5, 6, 3, 5, 3, 4, 4, 9,
        4, 8, 7, 0, 3, 7, 8, 8, 1, 6, 4, 0, 9, 7, 8, 6, 3, 6, 9, 6, 4, 7, 3, 3,
        5, 8, 5, 8, 0, 5, 1, 2, 5, 4, 3, 6, 6, 8, 9, 2, 8, 7, 8, 7, 8, 8, 6, 7,
        5, 8, 5, 2, 4, 4, 9, 0, 5, 1, 8, 9, 6, 5, 7, 7, 3, 5, 3, 5, 4, 9, 0, 7,
        9, 0, 2, 1, 3, 9, 3, 3, 8, 9, 8, 6, 6, 6, 7, 5, 8, 5, 0, 5, 8, 6, 1, 8,
        9, 6, 3, 7, 8, 2, 9, 0, 5, 8, 2, 9, 0, 2, 5, 7, 0, 6, 5, 1, 5, 9, 1, 9,
        5, 7, 0, 4, 7, 6, 6, 5, 7, 3, 3, 4, 0, 6, 4, 1, 1, 4, 8, 9, 2, 5, 3, 8,
        4, 0, 2, 0, 1, 9, 3, 4, 8, 3, 9, 5, 7, 9, 1, 1, 8, 3, 7, 1, 0, 4, 5, 7,
        8, 5, 5, 8, 0, 6, 5, 8, 6, 2, 0, 4, 5, 3, 8, 0, 6, 2, 4, 7, 7, 8, 1, 2,
        0, 9, 6, 3, 2, 1, 2, 5, 2, 7, 3, 3, 3, 8, 3, 2, 2, 3, 8, 1, 5, 9, 3, 9,
        1, 1, 0, 6, 2, 0, 2, 6, 7, 0, 2, 8, 5, 1, 9, 2, 1, 3, 5, 0, 7, 2, 3, 1,
        3, 9, 7, 8, 5, 6, 1, 4, 6, 0, 1, 2, 5, 0, 6, 5, 7, 9, 1, 5, 2, 9, 7, 2,
        4, 8, 6, 4, 7, 8, 1, 1, 1, 8, 7, 4, 8, 9, 5, 0, 6, 8, 7, 4, 3, 5, 5, 6,
        5, 7, 1, 0, 4, 2, 1, 4, 6, 6, 4, 3, 6, 3, 9, 5, 1, 0, 2, 5, 9, 8, 4, 9,
        7, 0, 6, 0, 1, 1, 9, 9, 9, 7, 4, 5, 1, 9, 5, 6, 3, 2, 6, 6, 3, 7, 0, 3,
        0, 4, 5, 4, 8, 1, 8, 0, 1, 3, 0, 8, 8, 4, 8, 8, 2, 2, 0, 3, 9, 6, 3, 9,
        6, 7, 6, 6, 8, 2, 7, 5, 8, 2, 5, 9, 7, 5, 7, 2, 5, 4, 7, 0, 5, 4, 7, 4,
        3, 7, 1, 8, 4, 0, 5, 3, 2, 1, 9, 5, 5, 9, 6, 2, 6, 9, 9, 6, 3, 8, 5, 9,
        3, 7, 8, 3, 7, 6, 2, 5, 4, 2, 9, 5, 6, 7, 1, 3, 4, 9, 7, 5, 8, 8, 2, 6,
        5, 0, 4, 7, 8, 7, 6, 4])], 'output': None, 'loss': None}